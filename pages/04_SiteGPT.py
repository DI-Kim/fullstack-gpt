from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import streamlit as st


def parse_page(soup):
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return (
        str(soup.get_text())
        .replace("\n", " ")
        .replace("\xa0", " ")
        .replace("CloseSearch Submit Blog", "")
        .replace("View all articles ", "")
    )


@st.cache_data(show_spinner="Loading website...")
def load_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000, chunk_overlap=200
    )
    loader = SitemapLoader(
        # filter_urlsë¥¼ í†µí•´ ì›í•˜ëŠ” siteë§Œ docsì— ë„£ì„ ìˆ˜ ìˆìŒ
        # ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•´ì„œ ì›í•˜ëŠ” urlë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ.
        # ex) ^(.*\/blog\/).* ëŠ” /blog/ ê°€ urlì— ì¡´ì¬í•˜ë©´ docsì— ì¶”ê°€ |  ?! ëŠ” ë°˜ëŒ€ì˜ ì˜ë¯¸ (ë¶€ì •)
        url,
        filter_urls=[r"^(.*\/blog\/).*"],
        parsing_function=parse_page,
    )
    # request íšŸìˆ˜ë¥¼ ì¡°ì •í•´(ëŠë¦¬ê²Œ) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì°¨ë‹¨ë‹¹í•˜ëŠ” ê²ƒì„ ë§‰ì„ ìˆ˜ ìˆìŒ.
    # loader.requests_per_second = 1
    docs = loader.load_and_split(text_splitter=splitter)
    return docs


st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)


st.markdown(
    """
    # SiteGPT
            
    Ask questions about the content of a website.
            
    Start by writing the URL of the website on the sidebar.
"""
)

with st.sidebar:
    url = st.text_input("Write down a URL", placeholder="https://example.com")

if url:
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a Sitemap URL")
    else:
        docs = load_website(url)
        st.write(docs)
